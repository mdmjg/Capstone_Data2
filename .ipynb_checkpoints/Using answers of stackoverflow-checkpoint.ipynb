{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esprima\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data/capstone_scraped.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    result = \"\"\n",
    "    for word in tokens:\n",
    "        result += word\n",
    "        result += \",\"\n",
    "    result = result[:-1]\n",
    "    result += \"\"\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(answer):\n",
    "    try:\n",
    "        esprima.parseScript(answer)\n",
    "        tokenized = esprima.tokenize(answer)\n",
    "        result = \"\"\n",
    "        for token in tokenized:\n",
    "            result += token.value\n",
    "            result += \",\"\n",
    "        result = result[:-1]\n",
    "        print(\"no error\")\n",
    "        return result\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"answer_text_tokenized\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = data['question_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"javascript_tokenized\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in question_ids:\n",
    "    answer_text = data['answer_text'][data['question_id'] == question].unique()[0]\n",
    "    result = tokenize_word(answer_text)\n",
    "    data['answer_text_tokenized'][data['question_id'] == question] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in question_ids:\n",
    "    js_code = data['javascript_code'][data['question_id'] == question].unique()[0]\n",
    "    if js_code is not None:\n",
    "        result = tokenize(js_code)\n",
    "        data['javascript_tokenized'][data['question_id'] == question] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_df = data.filter(['answer_text','javascript_code','javascript_tokenized', 'answer_text_tokenized'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_df[\"repo\"] = \"\"\n",
    "answer_text_df[\"path\"] = \"\"\n",
    "answer_text_df[\"func_name\"] = \"\"\n",
    "answer_text_df[\"url\"] = \"\"\n",
    "answer_text_df[\"sha\"] = \"\"\n",
    "answer_text_df[\"partition\"] = \"\"\n",
    "# new_df[\"original_string\"] = \"\"\n",
    "answer_text_df[\"language\"] = \"javascript\"\n",
    "answer_text_df[\"original_string\"] = answer_text_df[\"javascript_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_df.rename(columns={'javascript_code':'code'}, inplace=True)\n",
    "answer_text_df.rename(columns={'javascript_tokenized':'code_tokens'}, inplace=True)\n",
    "answer_text_df.rename(columns={'answer_text':'docstring'}, inplace=True)\n",
    "answer_text_df.rename(columns={'answer_text_tokenized':'docstring_tokens'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_df = answer_text_df[~answer_text_df['code'].isnull()]\n",
    "answer_text_df = answer_text_df[~answer_text_df['code_tokens'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = answer_text_df['docstring'].unique()\n",
    "for question in question_ids:\n",
    "    answer_text = answer_text_df['docstring'][answer_text_df['docstring'] == question].unique()[0]\n",
    "    result = tokenize_word(answer_text)\n",
    "    if result is None:\n",
    "        print(\"None\")\n",
    "    answer_text_df['docstring_tokens'][answer_text_df['docstring'] == question] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(tokenized_words):\n",
    "    # in the form of [hsjsjs, hsdjjsd]\n",
    "#     print(tokenized_words)\n",
    "    if tokenized_words:\n",
    "        tokenized_words = tokenized_words.split(\",\")\n",
    "    \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = answer_text_df.to_dict(orient='records') #list of dictionaries\n",
    "for single in data_dict:\n",
    "    single[\"code_tokens\"] = clean_token(single[\"code_tokens\"])\n",
    "    single[\"docstring_tokens\"] = clean_token(single[\"docstring_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(data_dict)\n",
    "training_length = int(length*0.7)\n",
    "validation_length = int(training_length + length*0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/answer_text/train.jsonl', 'w') as outfile:\n",
    "    for single in data_dict[:training_length]:\n",
    "        test = json.dumps(single)\n",
    "#         print(test)\n",
    "        outfile.write(test)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/answer_text/valid.jsonl', 'w') as outfile:\n",
    "    for single in data_dict[training_length:validation_length]:\n",
    "        test = json.dumps(single)\n",
    "#         print(test)\n",
    "        outfile.write(test)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/answer_text/test.jsonl', 'w') as outfile:\n",
    "    for single in data_dict[validation_length:]:\n",
    "        test = json.dumps(single)\n",
    "#         print(test)\n",
    "        outfile.write(test)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
